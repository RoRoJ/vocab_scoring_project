{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating frequency distributions from the BNC corpus\n",
    "\n",
    "This notebook shows how NLTK's FrequencyDistrbution function was used with the BNC corpus to create dictionaries mapping both single word vocabulary items and MWEs to their frequencies in the BNC. These dictionaries were saved as objects with the pickle module, for use by the FrequencyFeature and POSFrequencyFeature classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnc_reader = BNCCorpusReader(root=\"/Users/rowena/Documents/MSC/Project/BNC/2554/download/Texts/\", fileids=r'[A-K]/\\w*/\\w*\\.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/19201290/how-to-save-a-dictionary-to-a-file\n",
    "#as the frequency distribution dictionaries take so long to process, we use pickle to save these objects after creation\n",
    "def save_obj(obj, name ):\n",
    "    with open(r'C:/Users/rowena/Documents/MSC/Project/obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(r'C:/Users/rowena/Documents/MSC/Project/obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Single Word Frequency Distributions From the BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple frequency distribution\n",
    "bnc_words = bnc_reader.words() \n",
    "freqdist_bnc = nltk.FreqDist(word.lower() for word in bnc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#POS-tagged single word frequency distribution (using BNC's own POS tags)\n",
    "bnc_words_tagged = bnc_reader.tagged_words()\n",
    "bnc_words_tagged_lower=[]\n",
    "for item in bnc_words_tagged:\n",
    "    bnc_words_tagged_lower.append((item[0].lower(), item[1]))\n",
    "freqdist_bnc_tagged_lower = nltk.FreqDist(bnc_words_tagged_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#broader POS-tagged single word frequency distribution (using my mapping BNC's POS tags to my own broader POS tags)\n",
    "tag_dict = pd.read_excel('tool/files/tag_mapping.xlsx', sheet_name='Sheet2', usecols='A,B', index_col=0, header=0).to_dict()\n",
    "tag_dict = tag_dict['MY CAT']\n",
    "text_maptagged_entities=[]\n",
    "bnc_words_tagged_lower_mapped=[]\n",
    "for item in bnc_words_tagged_lower:\n",
    "    bnc_words_tagged_lower_mapped.append((item[0], tag_dict[item[1][0:3]]))\n",
    "freqdist_bnc_maptagged_lower = nltk.FreqDist(bnc_words_tagged_lower_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLTK POS-tagged single word frequency distribution (using NLTK's POS tagger)\n",
    "bnc_sents = bnc_reader.sents() \n",
    "bnc_words_nltktagged_lower=[]\n",
    "for sent in bnc_sents:\n",
    "    try:\n",
    "        for item in nltk.pos_tag(sent):\n",
    "            bnc_words_nltktagged_lower.append((item[0].lower(), item[1]))\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqdist_bnc_nltktagged_lower = nltk.FreqDist(tuple(item) for item in bnc_words_nltktagged_lower_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of MultiWord Frequency Distribution From the BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list which holds MWEs from the English wiktionary json file. This is our MWE lexicon.\n",
    "import json\n",
    "json_data = []\n",
    "for line in open(r'C:\\Users\\rowena\\Documents\\MSC\\Project\\PVs_Exprs\\enwikt.json', \"r\"):\n",
    "    json_data.append(json.loads(line))\n",
    "mwes = []\n",
    "for line in json_data:\n",
    "    mwes.append(dict(line)['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialise lemmatizer and detokenizer to use in the MWE extracter\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "detokenizer = MosesDetokenizer()\n",
    "#helper method to find position of ngrams in sentence\n",
    "#https://stackoverflow.com/questions/33393402/how-to-find-position-of-an-ngram-in-a-sentence\n",
    "def ngram_index(words, ngram):\n",
    "    return list(nltk.ngrams(words, len(ngram))).index(tuple(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making the multiword frequency distribution\n",
    "\n",
    "bnc_sents = bnc_reader.sents()\n",
    "bnc_sents_tagged = bnc_reader.tagged_sents()\n",
    "\n",
    "#extract mwes from text, put in found_ngrams list, and set the index to show where these occur\n",
    "found_ngrams=[]\n",
    "for i in range(len(bnc_sents)):\n",
    "    tokenized = bnc_sents[i]\n",
    "    tokenized_tagged = bnc_sents[i]\n",
    "    tokenized_for_inf = bnc_sents.copy()\n",
    "    sentence_index = [True] * (len(tokenized))\n",
    "    \n",
    "    #run through the tokenized text and change all verbs to infinitves in the inf version\n",
    "    for i in range(len(tokenized_for_inf)):\n",
    "        if tokenized_tagged[i][1]=='VERB':\n",
    "            tokenized_for_inf[i]=lemmatizer.lemmatize(tokenized_for_inf[i], 'v')\n",
    "    joined_string_orig=detokenizer.detokenize(tokenized, return_str=True)\n",
    "    joined_string_inf=detokenizer.detokenize(tokenized_for_inf, return_str=True)\n",
    "    \n",
    "    #run through lexicon of mwes and ...\n",
    "    for element in mwes:\n",
    "        joined=' '.join(element)\n",
    "        if joined in joined_string_orig:\n",
    "            sentence_ngrams=list(nltk.ngrams(tokenized, len(element)))\n",
    "            sentence_ngrams_index=list(nltk.ngrams(sentence_index, len(element)))\n",
    "            #...check each one against each ngram in current sentence\n",
    "            for n in range(len(sentence_ngrams)):\n",
    "                if tuple(element)==sentence_ngrams[n] and True in sentence_ngrams_index[n]:\n",
    "                    found_ngrams.append(sentence_ngrams[n])\n",
    "                    ngram_length=len(sentence_ngrams[n])\n",
    "                    for q in range(ngram_length):\n",
    "                        sentence_index[q+(ngram_index(tokenized,sentence_ngrams[n]))]=False\n",
    "                    break\n",
    "        elif joined in joined_string_inf:\n",
    "            sentence_ngrams=list(nltk.ngrams(tokenized_for_inf, len(element)))\n",
    "            sentence_ngrams_index=list(nltk.ngrams(sentence_index, len(element)))\n",
    "            #...check each one against each ngram in current sentence\n",
    "            for n in range(len(sentence_ngrams)):\n",
    "                if tuple(element)==sentence_ngrams[n] and True in sentence_ngrams_index[n]:\n",
    "                    found_ngrams.append(sentence_ngrams[n])\n",
    "                    ngram_length=len(sentence_ngrams[n])\n",
    "                    for q in range(ngram_length):\n",
    "                        sentence_index[q+(ngram_index(tokenized_for_inf,sentence_ngrams[n]))]=False\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform all found mwes in the BNC to lower case\n",
    "mylistlower=[]\n",
    "for sublist in found_ngrams=[]:\n",
    "    mylowermwes=[]\n",
    "    for i in range(len(sublist)):\n",
    "        mylowermwes.append(sublist[i].lower())\n",
    "    mylistlower.append(tuple(mylowermwes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mwesfreqdist=nltk.FreqDist(mylistlower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creation of Combined Single Word and MWE Frequency Distribution\n",
    "This is converted to a dictionary object and saved with pickle for use by the FrequencyFeature class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_bnc_freqdist_dict_lower = dict(freqdist_bnc.update(mwesfreqdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(combined_bnc_freqdist_dict_lower, 'combined_bnc_freqdist_dict_lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Combined POS-taggedSingle Word and MWE Frequency Distribution\n",
    "This is converted to a dictionary object and saved with pickle for use by the POSFrequencyFeature class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_mwes_dict=[]\n",
    "for item in dict(mwesfreqdist):\n",
    "    tagged_mwes_dict.append( ((item, 'MWE'), mwesfreqdist[item]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_tagged_bnc_freqdict_lower = dict(freqdist_bnc_nltktagged_lower).update(dict(tagged_mwes_dict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(combined_tagged_bnc_freqdict_lower, 'combined_tagged_bnc_freqdict_lower')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
