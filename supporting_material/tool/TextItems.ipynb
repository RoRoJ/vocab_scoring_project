{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class TextItems:\n",
    "    \n",
    "    \"\"\"\n",
    "    This class facilitates the extraction of vocabulary items from a text file\n",
    "\n",
    "    Attributes:\n",
    "        text_sentences: list of sentence tokens from the text\n",
    "    \"\"\"\n",
    "    \n",
    "    #open text with plain text reader, return its tokenized sentences, and count of tokenized words\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        The constructor for the Textitems class.\n",
    "        \n",
    "        Parameters:\n",
    "           filename (string): The filepath for the text file  \n",
    "        \"\"\"\n",
    "        with open(filename, 'r', encoding=\"utf8\") as myfile:\n",
    "            text_plain = myfile.read()\n",
    "            text_plain = text_plain.replace('\\ufeff', '')\n",
    "            text_plain = text_plain.replace('\\n', ' ')\n",
    "            self.text_sentences=nltk.tokenize.sent_tokenize(text_plain)\n",
    "\n",
    "    @staticmethod\n",
    "    def ngram_index(words, ngram):\n",
    "        \"\"\"\n",
    "        Function to compute the position of an n-gram in a sentence.\n",
    "        https://stackoverflow.com/questions/33393402/how-to-find-position-of-an-ngram-in-a-sentence\n",
    "        \n",
    "        Parameters:\n",
    "           words (list): A list of word tokens(constituting a sentence)\n",
    "           ngram (list): A list of word tokens (constituting an ngram potentially within the sentence)\n",
    "        \n",
    "        Returns:\n",
    "            An integer corresponding to the starting index  of the ngram in the (list of) words\n",
    "        \"\"\"\n",
    "        return list(nltk.ngrams(words, len(ngram))).index(tuple(ngram))\n",
    "    \n",
    "    #takes a sentence-tokenized text and master list of mwexps as args: must be in form [[mwe1],[mwe2],[mwn]]\n",
    "    #returns one of two types of entity list:\n",
    "    #1. All mwexpss and (mutually exclusive) single tokens found in the text\n",
    "    #2. The above, with POS tags\n",
    "\n",
    "    @staticmethod\n",
    "    def get_text_items(text_sentences, return_option, case_option, exclude_nondict_option):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to extract vocabulary items and POS tags from a list of sentences\n",
    "        \n",
    "        Parameters:\n",
    "            text_sentences (list): A list of sentence tokens\n",
    "            return_option (string): Determines whether POS tags should be returned with the vocab items\n",
    "            case_option (string): Determines whether the sentence tokens should be converted to lower case\n",
    "            exclude_nondict_option (string): Determines whether non-dictionary items should be returned\n",
    "        \n",
    "        Returns:\n",
    "            A list comprising single word tokens and tuple tokens of MWEs found within the text OR\n",
    "            A list of tuples (comprising the above with POS tags)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #load list of mwes\n",
    "        with open(r'obj/mwes_list.pkl', 'rb') as f:\n",
    "            mwes_list= pickle.load(f)\n",
    "        \n",
    "        #transform mwes in the list to lower case\n",
    "        if case_option=='all_lower':\n",
    "            mwes_list_lower=[]\n",
    "            mwes_list_entry_lower=[]\n",
    "            for item in mwes_list:\n",
    "                for word in item:\n",
    "                    mwes_list_entry_lower.append(word.lower())\n",
    "                mwes_list_lower.append(mwes_list_entry_lower)\n",
    "                mwes_list_entry_lower=[]\n",
    "            mwes_list=mwes_list_lower\n",
    "      \n",
    "        #initialise detokenizer and lemmatizer\n",
    "        detokenizer = MosesDetokenizer()\n",
    "        lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "        #count the number of tokens in the whole text\n",
    "        tokens_in_text=0\n",
    "        for sentence in text_sentences:\n",
    "            tokenized = nltk.tokenize.word_tokenize(sentence)\n",
    "            tokens_in_text+=len(tokenized)\n",
    "    \n",
    "        #create a text index, with a boolean value corresponding to each individual token\n",
    "        text_index = [True] * tokens_in_text\n",
    "\n",
    "        found_ngrams=[] #this holds the mwexprs/ngrams found in the text\n",
    "        pos_document=[] #this holds the pos-tagged tokens in the text\n",
    "        token_counter=0 #this accumulates the number of tokens processed in each loop \n",
    "    \n",
    "        #loops through each sentence in the text\n",
    "        for i in range(len(text_sentences)):\n",
    "            tokenized = nltk.tokenize.word_tokenize(text_sentences[i]) #tokenises the current sentence\n",
    "            tokenized_for_inf = nltk.tokenize.word_tokenize(text_sentences[i]) #copy of above for an infinitive search\n",
    "            tokenized_tagged = nltk.pos_tag(tokenized)  #tokenises and POS-tags the current sentence\n",
    "    \n",
    "            if case_option=='all_lower':\n",
    "                tokenized = [item.lower() for item in tokenized]\n",
    "                tokenized_for_inf = [item.lower() for item in tokenized_for_inf]\n",
    "                tokenized_tagged = [(item[0].lower(), item[1]) for item in tokenized_tagged]\n",
    "        \n",
    "            pos_document.append(tokenized_tagged)\n",
    "    \n",
    "            #run through the tokenized sentence. If a verb is detected, change the verb to the \n",
    "            #infinitive in the tokenized_for_inf sentence\n",
    "            for i in range(len(tokenized_for_inf)):\n",
    "                if tokenized_tagged[i][1][0]=='V':\n",
    "                    tokenized_for_inf[i]=lemmatizer.lemmatize(tokenized[i], 'v')\n",
    "    \n",
    "            #create two joined strings of the sentence, one in its original form, one with the verbs in the infinitve\n",
    "            joined_string_orig=detokenizer.detokenize(tokenized, return_str=True)\n",
    "            joined_string_inf=detokenizer.detokenize(tokenized_for_inf, return_str=True)\n",
    "    \n",
    "            #loop through the master list of multiword expressions\n",
    "            for element in mwes_list:\n",
    "                joined=' '.join(element) #join the mwe into a string with spaces\n",
    "        \n",
    "                #for speed improvements, check whether the joined mwe is in the joined original string. Skip rest if not.\n",
    "                if joined in joined_string_orig:\n",
    "                    sentence_ngrams=list(nltk.ngrams(tokenized, len(element)))\n",
    "                    sentence_ngrams_index=list(nltk.ngrams(text_index[(token_counter):(token_counter+len(tokenized))], len(element)))\n",
    "                    #this 2nd variable copies the current state of the text_index for this set of ngrams\n",
    "            \n",
    "                    #...check each mwe found in the joined sentence against each ngram in the tokenised sentence\n",
    "                    for n in range(len(sentence_ngrams)):\n",
    "                        #only let it match if at least one of the tokens in the ngram hasn't been used in another mwe match\n",
    "                        if tuple(element)==sentence_ngrams[n] and (True in sentence_ngrams_index[n]):\n",
    "                            found_ngrams.append(sentence_ngrams[n]) #append the found mwe ngram to the list\n",
    "                            ngram_length=len(sentence_ngrams[n])\n",
    "                            for q in range(ngram_length): #this loop sets the booleans in the text index that correspond to the ngram, to False\n",
    "                                text_index[q+token_counter+(TextItems.ngram_index(tokenized,sentence_ngrams[n]))]=False\n",
    "                            break #once an mwe is found in the sentence ngrams, the rest of its ngrams aren't checked\n",
    "                    \n",
    "                #for speed improvements, check whether the joined mwe is in the joined verbs-in-infinitve string. Skip rest if not.\n",
    "                elif joined in joined_string_inf:\n",
    "                    sentence_ngrams=list(nltk.ngrams(tokenized_for_inf, len(element))) \n",
    "                    sentence_ngrams_index=list(nltk.ngrams(text_index[(token_counter):(token_counter+len(tokenized))], len(element)))           \n",
    "            \n",
    "                    for n in range(len(sentence_ngrams)): \n",
    "                        if (tuple(element)==sentence_ngrams[n]) and (True in sentence_ngrams_index[n]):\n",
    "                            found_ngrams.append(sentence_ngrams[n])\n",
    "                            ngram_length=len(sentence_ngrams[n])\n",
    "                            for q in range(ngram_length):\n",
    "                                text_index[q+token_counter+(TextItems.ngram_index(tokenized_for_inf,sentence_ngrams[n]))]=False\n",
    "                            break\n",
    "                    \n",
    "            token_counter+=len(tokenized) #increment the token counter by the length of this sentence's tokens\n",
    "    \n",
    "        #turns the list of sublists of tuples that is the current pos_doc, into a flat list of tuples\n",
    "        flat_pos_doc = [item for sublist in pos_document for item in sublist]\n",
    "        if len(flat_pos_doc)!=len(text_index):\n",
    "                print('Error when flattening and index-checking entities')\n",
    "    \n",
    "        #appends valid single word tokens (not with corresponding False ie used by a MWES) to entity list\n",
    "        entities=[]\n",
    "        for i in range(len(flat_pos_doc)):\n",
    "            if text_index[i]==True and re.search('^d*[A-z]', flat_pos_doc[i][0]) and flat_pos_doc[i][0]!='[' and flat_pos_doc[i][0]!=']':\n",
    "                entities.append(flat_pos_doc[i])\n",
    "        #appends found mwes to entity list\n",
    "        for item in found_ngrams:\n",
    "            entities.append((item, 'MWE'))\n",
    "        \n",
    "        #excludes words that aren't either an MWE, a stopword, hyphenated or in Wordnet\n",
    "        if exclude_nondict_option=='yes':\n",
    "            extended_stopwords=[\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"whichever\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "            entities_2=[]\n",
    "            for item in entities:\n",
    "                if item[0]=='s':\n",
    "                    pass\n",
    "                elif item[1]==\"MWE\":\n",
    "                    entities_2.append(item)\n",
    "                elif '-' in item[0]:\n",
    "                    entities_2.append(item)\n",
    "                elif(wordnet.synsets(item[0])) or (item[0] in extended_stopwords):\n",
    "                    entities_2.append(item)\n",
    "            entities=entities_2\n",
    "    \n",
    "        if return_option=='tagged':\n",
    "            return [item for item in entities]\n",
    "    \n",
    "        elif return_option=='untagged':\n",
    "            return [item[0] for item in entities]\n",
    "        \n",
    "    @staticmethod\n",
    "    def map_text_tags(text_tagged_items):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to map NLTK POS tags to broader POS tags\n",
    "        \n",
    "        Parameters:\n",
    "            text_tagged_items(list): A list of tuples (vocabitem, NLTKPOStag)\n",
    "        \n",
    "        Returns:\n",
    "            A list of tuples (vocabitem, broadPOStag)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #read list of tag mappings from excel:\n",
    "        tag_dict = pd.read_excel('files/tag_mapping.xlsx', usecols='A,B', index_col=0, header=0).to_dict()\n",
    "        tag_dict = tag_dict['MY CAT']\n",
    "        #perform the mapping:\n",
    "        text_maptagged_entities=[]\n",
    "        for item in text_tagged_items:\n",
    "            mapped_item=(item[0], tag_dict[item[1]])\n",
    "            text_maptagged_entities.append(mapped_item)\n",
    "        return text_maptagged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
