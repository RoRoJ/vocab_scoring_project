{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "class L1SimilarityFeature:\n",
    "    \n",
    "    \"\"\"\n",
    "    This class computes L1 Similarity Feature scores for vocabulary items\n",
    "    Currently configured to French only\n",
    "    Other languages possible with the addition of more dictionaries\n",
    "\n",
    "    Attributes:\n",
    "        french_dict_simple: a dictionary which maps english vocabulary items to french translations\n",
    "                            and their Levenshtein scores\n",
    "        french_dict_pos: a more sophisticated version of the above, incorporating pos tags into the mapping\n",
    "    \"\"\"\n",
    "    \n",
    "    #open text with plain text reader, return its tokenized sentences, and count of tokenized words\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        The constructor for the class. Initiates the dictionary objects.\n",
    "        \"\"\"\n",
    "        \n",
    "        #new, simple dictionary (parentheses text removed)\n",
    "        with open(r'C:obj/french_dict_simple.pkl', 'rb') as f:\n",
    "            french_dict=pickle.load(f)\n",
    "        french_dict_lower=[]\n",
    "        for key in french_dict:\n",
    "            if type(key)!=tuple:\n",
    "                french_dict_lower.append((key.lower(), french_dict[key]))\n",
    "            if type(key)==tuple:\n",
    "                lower_tuple=[]\n",
    "                for component in key:\n",
    "                    lower_tuple.append(component.lower())\n",
    "                french_dict_lower.append((tuple(lower_tuple), french_dict[key]))\n",
    "        self.french_dict_simple=dict(french_dict_lower)\n",
    "        \n",
    "        #new, POS complex dictionary (parentheses text removed)\n",
    "        with open(r'C:obj/french_dict_pos.pkl', 'rb') as f:\n",
    "            french_dict=pickle.load(f)  \n",
    "        \n",
    "        tag_dict=pd.read_excel('files/tag_mapping_forfrenchdict.xlsx', sheet_name='tag_mapping_frdict', usecols='A,B', index_col=0, header=0).to_dict()\n",
    "        tag_dict = tag_dict['MAPTAG']\n",
    "            \n",
    "        french_dict_lower=[]\n",
    "        for key in french_dict:\n",
    "            #check whether word part of key is single word or tuple\n",
    "            if type(key[0])!=tuple:\n",
    "                #if it's single word\n",
    "                french_dict_lower.append(((key[0].lower(),tag_dict[key[1]]), french_dict[key]))\n",
    "            if type(key[0])==tuple:\n",
    "                lower_tuple=[]\n",
    "                for component in key[0]:\n",
    "                    lower_tuple.append(component.lower())\n",
    "                french_dict_lower.append( ((tuple(lower_tuple), tag_dict[key[1]]), french_dict[key]) )\n",
    "        self.french_dict_pos=dict(french_dict_lower)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_frenchscore_table_simple(text_untagged_items, french_dict):\n",
    "        \"\"\"\n",
    "        Calculates normalised L1 Similarity scores for vocab items using the French dictionary\n",
    "\n",
    "        Params:\n",
    "            text_untagged_items(list): list of vocabulary items\n",
    "            french_dict: dictionary which maps vocabulary items to frequencies in the BNC\n",
    "        \n",
    "        Returns: pandas dataframe of vocabulary items and their L1 Similarity scores\n",
    "        \"\"\"\n",
    "        base_table = pd.DataFrame(text_untagged_items)\n",
    "        base_table.columns=['word_in_text']\n",
    "        base_table['score'] = base_table['word_in_text'].map(dict(french_dict))\n",
    "\n",
    "        porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        stemmer2 = nltk.SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "        lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "        base_table['stemmed']=''\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']) and type(row['word_in_text'])!=tuple:\n",
    "                row['stemmed']= porter_stemmer.stem(row['word_in_text'])\n",
    "        base_table['stemmed_score'] = base_table['stemmed'].map(dict(french_dict))\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                row['score']=row['stemmed_score']\n",
    "\n",
    "        base_table['capitalised']=''\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']) and type(row['word_in_text'])!=tuple:\n",
    "                row['capitalised']=row['word_in_text'].capitalize()\n",
    "        base_table['capitalised_score'] = base_table['capitalised'].map(dict(french_dict))\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                row['score']=row['capitalised_score']\n",
    "    \n",
    "        base_table['lemmatized']=''\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']) and type(row['word_in_text'])!=tuple:\n",
    "                row['lemmatized'] = lemmatizer.lemmatize(row['word_in_text'], 'v')\n",
    "        base_table['lemmatized_score'] = base_table['lemmatized'].map(dict(french_dict))\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                row['score']=row['lemmatized_score']\n",
    "    \n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']) and type(row['word_in_text'])==tuple:\n",
    "                row['score']=('unfound_mwes', 1.0)\n",
    "            if type(row['word_in_text'])!=tuple:\n",
    "                if pd.isna(row['score']) and bool(re.search('^[A-z]+$', row['word_in_text']))==False:\n",
    "                    row['score']=('punct/number', 1.0)\n",
    "\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']) and type(row['word_in_text'])==tuple:\n",
    "                row['score']=('unfound_mwes', 1.0)\n",
    "            if type(row['word_in_text'])!=tuple:\n",
    "                if pd.isna(row['score']) and bool(re.search('^[A-z]+$', row['word_in_text']))==False:\n",
    "                    row['score']=('punct/number', 1.0)\n",
    "        \n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                row['score']=('unfound single word', 1.0)\n",
    "        \n",
    "        base_table['french_score']=''\n",
    "        for index, row in base_table.iterrows():\n",
    "            row['french_score']=row['score'][1]\n",
    "\n",
    "        return base_table\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_frenchscore_table_pos(text_maptagged_items, french_dict_pos, french_dict_simple):\n",
    "        \"\"\"\n",
    "        Calculates normalised L1 Similarity scores for vocab items using the French dictionary\n",
    "        Enhances previous method's performnace by incorporating POS tags\n",
    "        Params:\n",
    "            text_maptagged_items(list): list of vocabulary items with broad POS tags\n",
    "            french_dict_pos: dictionary which maps vocab items + pos tags to French translations\n",
    "            french_dict_simple: dictionary which maps vocab items alone to French translations\n",
    "        \n",
    "        Returns: pandas dataframe of vocabulary items and their L1 Similarity Scores\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        base_table = pd.DataFrame(text_maptagged_items)\n",
    "        base_table.columns=['word_in_text', 'tag']\n",
    "        base_table['word_and_tag']=''\n",
    "        for index, row in base_table.iterrows():\n",
    "            row['word_and_tag']=(row['word_in_text'], row['tag'])\n",
    "        \n",
    "        #first attempt to find a score by matching wholesale word+tag to an entry in french_dict with pos tags\n",
    "        base_table['score'] = base_table['word_and_tag'].map(dict(french_dict_pos))\n",
    "\n",
    "        porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        stemmer2 = nltk.SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "        lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "        base_table['stemmed']=''\n",
    "        base_table['stemmed_and_tag']=''\n",
    "        base_table['lemmatized']=''\n",
    "        base_table['lemmatized_and_tag']=''\n",
    "        \n",
    "        #second and third attempts to find a score by matching stemmed then lemmatised word+tag to an entry in french_dict with pos tags\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']) and type(row['word_in_text'])!=tuple:\n",
    "                row['stemmed']= porter_stemmer.stem(row['word_in_text'])\n",
    "                row['stemmed_and_tag']=(row['stemmed'], row['tag'])\n",
    "                if row['tag']=='VERB':\n",
    "                    row['lemmatized'] = lemmatizer.lemmatize(row['word_in_text'], 'v')\n",
    "                elif (row['tag']=='NOUN') or (row['tag']=='PRP_NOUN'):\n",
    "                    row['lemmatized'] = lemmatizer.lemmatize(row['word_in_text'], 'n')\n",
    "                elif (row['tag']=='ADJ_ADV') and (row['word_in_text'].endswith('ly')):\n",
    "                    row['lemmatized'] = row['word_in_text'][:-2]\n",
    "                row['lemmatized_and_tag']=(row['lemmatized'], row['tag'])\n",
    "                \n",
    "        base_table['stemmed_tagscore'] = base_table['stemmed_and_tag'].map(dict(french_dict_pos))\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                row['score']=row['stemmed_tagscore']\n",
    "    \n",
    "        base_table['lemmatized_tagscore'] = base_table['lemmatized_and_tag'].map(dict(french_dict_pos))\n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                row['score']=row['lemmatized_tagscore']\n",
    "        \n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                #fourth attempt to find a score by matching word without tag to an entry in french_dict_simple\n",
    "                row['score']=(french_dict_simple.get(row['word_in_text']))\n",
    "            if pd.isna(row['score']):\n",
    "                #fifth attempt to find a score by matching stemmed word without tag to an entry in french_dict_simple\n",
    "                row['score']=french_dict_simple.get(row['stemmed'])\n",
    "            if pd.isna(row['score']):\n",
    "                #sixth attempt to find a score by matching lemmatized word without tag to an entry in french_dict_simple\n",
    "                row['score']=french_dict_simple.get(row['lemmatized'])\n",
    "        \n",
    "    \n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']) and type(row['word_in_text'])==tuple:\n",
    "                row['score']=('unfound_mwes', 1.0)\n",
    "            if type(row['word_in_text'])!=tuple:\n",
    "                if pd.isna(row['score']) and bool(re.search('^[A-z]+$', row['word_in_text']))==False:\n",
    "                    row['score']=('punct/number', 1.0)\n",
    "      \n",
    "        for index, row in base_table.iterrows():\n",
    "            if pd.isna(row['score']):\n",
    "                row['score']=('unfound single word', 1.0)\n",
    "        \n",
    "        base_table['french_score']=''\n",
    "        for index, row in base_table.iterrows():\n",
    "            row['french_score']=row['score'][1]\n",
    "\n",
    "        return base_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
